# NeuralNetwork - LibrerÃ­a de Deep Learning desde Cero

Este proyecto es una librerÃ­a de Deep Learning ligera y modular desarrollada Ã­ntegramente en Python y NumPy. A diferencia de frameworks de alto nivel como TensorFlow o PyTorch, **NeuralNetwork** implementa la matemÃ¡tica de las redes neuronales desde la base, permitiendo una comprensiÃ³n profunda de los algoritmos de retropropagaciÃ³n (Backpropagation), optimizaciÃ³n y funciones de activaciÃ³n.

EstÃ¡ diseÃ±ada para ser escalable y educativa, permitiendo la creaciÃ³n de arquitecturas personalizadas para resolver problemas de clasificaciÃ³n binaria y regresiÃ³n, como el clÃ¡sico problema XOR.

## ðŸš€ CaracterÃ­sticas Principales
* **Arquitectura Modular:**
    * **DiseÃ±o Orientado a Objetos:** SeparaciÃ³n lÃ³gica entre Neuronas, Capas (`Layer`) y el Orquestador (`NeuralNetwork`).
    * **Activaciones Flexibles:** ImplementaciÃ³n de funciones `Sigmoid`, `ReLU` y `LeakyReLU` intercambiables por capa.
* **OptimizaciÃ³n MatemÃ¡tica Avanzada:**
    * **InicializaciÃ³n de He:** InicializaciÃ³n inteligente de pesos para prevenir el desvanecimiento de gradientes en redes profundas.
    * **Funciones de PÃ©rdida (Loss Functions):** Soporte para `MSE` (Error CuadrÃ¡tico Medio) y `BinaryCrossEntropy` (EntropÃ­a Cruzada Binaria).
    * **PrevenciÃ³n de "Dying ReLU":** ImplementaciÃ³n de `LeakyReLU` para mantener el flujo de gradientes en valores negativos.
* **Entrenamiento Robusto:**
    * **Stochastic Gradient Descent (SGD):** OptimizaciÃ³n de pesos mediante descenso de gradiente.
    * **Data Shuffling:** Barajado automÃ¡tico de datos en cada Ã©poca para evitar mÃ­nimos locales y ciclos repetitivos.
* **Persistencia de Modelos:**
    * Sistema nativo para guardar (`save_model`) y cargar (`load_model`) redes entrenadas utilizando `pickle`.
* **VisualizaciÃ³n:**
    * IntegraciÃ³n con `matplotlib` para generar curvas de aprendizaje y monitorear la convergencia del error en tiempo real.

## ðŸ› ï¸ TecnologÃ­as Utilizadas
El proyecto utiliza un stack enfocado en el cÃ¡lculo numÃ©rico y la eficiencia matemÃ¡tica:
* **Lenguaje:** Python 3.10+
* **CÃ¡lculo NumÃ©rico:** NumPy (Ãlgebra lineal, operaciones matriciales).
* **Empaquetado:** Setuptools (Estructura de librerÃ­a instalable).
* **VisualizaciÃ³n:** Matplotlib (GrÃ¡ficos de curvas de pÃ©rdida).
* **Testing:** Unittest (Pruebas unitarias para neuronas, capas y pÃ©rdidas).

## ðŸ“‹ Pre-requisitos
AsegÃºrate de tener instalado y configurado lo siguiente:
* Python 3.8 o superior
* Git
* Virtualenv (recomendado)

## ðŸ”§ InstalaciÃ³n y ConfiguraciÃ³n
Sigue estos pasos para levantar el proyecto en tu entorno local:

1. **Clonar el repositorio:**
```bash
git clone [https://github.com/elJulioDev/Neural_Network.git](https://github.com/elJulioDev/Neural_Network.git)
cd neural_network
```

2. **Crear y activar un entorno virtual:**
```bash
python -m venv venv
# En Windows:
venv\Scripts\activate
# En macOS/Linux:
source venv/bin/activate
```

3. **Instalar dependencias:** Puedes instalarlo como un paquete local editable o instalar las dependencias directamente:
```bash
pip install -r requirements.txt
# O alternativamente para desarrollo:
pip install -e .
```

4. **Ejecutar Pruebas Unitarias:** Para asegurar que toda la matemÃ¡tica base funciona correctamente:
```bash
python -m unittest discover tests
```

5. **Ejecutar el ejemplo (XOR):** Entrena la red para resolver la compuerta lÃ³gica XOR:
```bash
python main.py
```

## ðŸ” Uso del Sistema
La librerÃ­a estÃ¡ diseÃ±ada para ser intuitiva. AquÃ­ tienes un ejemplo de cÃ³mo configurar una red para clasificaciÃ³n:
```bash
import numpy as np
from src.neural_network import NeuralNetwork
from src.activations import LeakyReLU, Sigmoid
from src.losses import BinaryCrossEntropy

# 1. Inicializar la red con funciÃ³n de pÃ©rdida
nn = NeuralNetwork(loss_function=BinaryCrossEntropy())

# 2. Definir Arquitectura
# Capa de entrada (2 neuronas) -> Oculta (4 neuronas, LeakyReLU)
nn.add_layer(num_neurons=4, input_size=2, activation=LeakyReLU())
# Capa de salida (1 neurona, Sigmoid)
nn.add_layer(num_neurons=1, activation=Sigmoid())

# 3. Entrenar
nn.train(X, y, epochs=10000, learning_rate=0.1)

# 4. Predecir
predicciones = nn.predict(X)
```

## ðŸ“‚ Estructura del Proyecto
```text
neural_network/
â”œâ”€â”€ src/                            # CÃ³digo fuente de la librerÃ­a
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ activations.py              # Funciones (Sigmoid, ReLU, LeakyReLU)
â”‚   â”œâ”€â”€ layer.py                    # LÃ³gica de capas y conexiÃ³n de neuronas
â”‚   â”œâ”€â”€ losses.py                   # Funciones de costo (MSE, CrossEntropy)
â”‚   â”œâ”€â”€ neural_network.py           # Orquestador principal y bucle de entrenamiento
â”‚   â””â”€â”€ neuron.py                   # LÃ³gica base de la neurona (pesos/bias)
â”œâ”€â”€ tests/                          # Pruebas Unitarias
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ test_layer.py
â”‚   â”œâ”€â”€ test_losses.py
â”‚   â””â”€â”€ test_neuron.py
â”œâ”€â”€ main.py                         # Script de demostraciÃ³n (Problema XOR)
â”œâ”€â”€ setup.py                        # ConfiguraciÃ³n de instalaciÃ³n del paquete
â”œâ”€â”€ requirements.txt                # Dependencias
â””â”€â”€ .gitignore                      # Archivos ignorados
```

## ðŸ‘¥ CrÃ©ditos
Este proyecto ha sido desarrollado por **Alexis GonzÃ¡lez** como parte de una investigaciÃ³n profunda sobre los fundamentos matemÃ¡ticos de la Inteligencia Artificial.

## ðŸ“„ Licencia
Este proyecto es de uso educativo y personal. Se distribuye bajo la licencia MIT.
